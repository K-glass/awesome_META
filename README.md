# awesome_META

## Contents

### Continual Learning

- **Vision**
  - **Pre-trained model-based**
    - **Survey**
      - -[ ] Continual Learning with Pre-Trained Models: A Survey (arXiv, Jan 2024) [[paper]](https://arxiv.org/abs/2401.16386)
    - **W/ Prompt**
      - -[x] L2P: Learning to Prompt for Continual Learning (CVPR, 2022) 
      - -[x] DualPrompt: Complementary Prompting for Rehearsal-free Continual Learning (ECCV, 2022) 
      - -[x] CODA-Prompt: COntinual Decomposed Attention-based Prompting for Rehearsal-Free Continual Learning (CVPR, 2023) 
      - -[ ] Generating Instance-level Prompts for Rehearsal-free Continual Learning (ICCV, 2023)
      - -[ ] POP: Prompt Of Prompts for Continual Learning (Arxiv, 2023)
      - -[ ] Steering Prototypes with Prompt-tuning for Rehearsal-free Continual Learning (WACV, 2024)
      - -[ ] KOPPA: Improving Prompt-based Continual Learning with Key-Query Orthogonal Projection and Prototype-based One-Versus-All (ICLR, 2024, underreview)
    - **W/o Prompt**
      - -[x] Revisiting Class-Incremental Learning with Pre-Trained Models: Generalizability and Adaptivity are All You Need (Arxiv, 2023)
      - -[ ] Read Between the Layers: Leveraging Intra-Layer Representations for Rehearsal-Free Continual Learning with Pre-Trained Models (Arxiv, 2023)
      - -[ ] RanPAC: Random Projections and Pre-trained Models for Continual Learning (NIPs, 2023)
      - -[ ] SLCA: Slow Learner with Classifier Alignment for Continual Learning on a Pre-trained Model (ICCV, 2023)
      - -[ ] When Prompt-based Incremental Learning Does Not Meet Strong Pretraining (ICCV, 2023)
  - **From-scratch model-based**
    - **Regularization**
    - **Model**
      - -[ ] Masked Autoencoders are Efficient Class Incremental Learners (ICCV, 2023) 
    - **Distillation**
    - **SSL**
      - -[ ] Instance and Category Supervision are Alternate Learners for Continual Learning(ICCV, 2023)
- **Vision Language**
  - -[ ] Climb: A Continual Learning Benchmark for Vision-and-Language Tasks (NIPs, 2022)
  - -[ ] Continual Vision-Language Representation Learning with Off-Diagonal Information (ICML, 2023)
  - -[x] CTP: Towards Vision-Language Continual Pretraining via Compatible Momentum Contrast and Topology Preservation (ICCV, 2023)
  - -[ ] Generative Negative Text Replay for Continual Vision-Language Pretraining (ECCV, 2022)
  - -[ ] Learning without Forgetting for Vision-Language Models (Arxiv, 2023)
  - -[x] Preventing Zero-Shot Transfer Degradation in Continual Learning of Vision-Language Models (ICCV, 2023)
  - -[x] Task-Attentive Transformer Architecture for Continual Learning of Vision-and-Language Tasks Using Knowledge Distillation (ACL, 2023)
  - -[ ] Parameterizing Context: Unleashing the Power of Parameter-Efficient Fine-Tuning and In-Context Tuning for Continual Table Semantic Parsing (NIPs, 2023)
  - -[ ] Donâ€™t Stop Learning: Towards Continual Learning for the CLIP Model
  - -[ ] CLIP MODEL IS AN EFFICIENT CONTINUAL LEARNER
  - -[ ] Select and Distill: Selective Dual-Teacher Knowledge Transfer for Continual Learning on Vision-Language Models
    
### Long-tail Learning

