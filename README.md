# awesome_META

## Contents

### Continual Learning
- **Vision**
  - **Pre-trained model-based**
    - **W/ Prompt**
      -[x] L2P: Learning to Prompt for Continual Learning (CVPR, 2022) 
      -[x] DualPrompt: Complementary Prompting for Rehearsal-free Continual Learning (ECCV, 2022) 
      -[x] CODA-Prompt: COntinual Decomposed Attention-based Prompting for Rehearsal-Free Continual Learning (CVPR, 2023) 
      - Generating Instance-level Prompts for Rehearsal-free Continual Learning (ICCV, 2023)
      - POP: Prompt Of Prompts for Continual Learning (Arxiv, 2023)
      - Steering Prototypes with Prompt-tuning for Rehearsal-free Continual Learning (WACV, 2024)
      - KOPPA: Improving Prompt-based Continual Learning with Key-Query Orthogonal Projection and Prototype-based One-Versus-All (ICLR, 2024, underreview)
    - **W/o Prompt**
      - Revisiting Class-Incremental Learning with Pre-Trained Models: Generalizability and Adaptivity are All You Need (Arxiv, 2023)
      - Read Between the Layers: Leveraging Intra-Layer Representations for Rehearsal-Free Continual Learning with Pre-Trained Models (Arxiv, 2023)
      - RanPAC: Random Projections and Pre-trained Models for Continual Learning (NIPs, 2023)
      - SLCA: Slow Learner with Classifier Alignment for Continual Learning on a Pre-trained Model (ICCV, 2023)
      - When Prompt-based Incremental Learning Does Not Meet Strong Pretraining (ICCV, 2023)
  - **From-scratch model-based**
    - **Regularization**
    - **Model**
      - Masked Autoencoders are Efficient Class Incremental Learners (ICCV, 2023) 
    - **Distillation**
    - **SSL**
      - Instance and Category Supervision are Alternate Learners for Continual Learning(ICCV, 2023)
- **Vision Language**
  - Climb: A Continual Learning Benchmark for Vision-and-Language Tasks (NIPs, 2022)
  - Continual Vision-Language Representation Learning with Off-Diagonal Information (ICML, 2023)
  - CTP: Towards Vision-Language Continual Pretraining via Compatible Momentum Contrast and Topology Preservation (ICCV, 2023)
  - Generative Negative Text Replay for Continual Vision-Language Pretraining (ECCV, 2022)
  - Learning without Forgetting for Vision-Language Models (Arxiv, 2023)
  - Preventing Zero-Shot Transfer Degradation in Continual Learning of Vision-Language Models (ICCV, 2023)
  - Task-Attentive Transformer Architecture for Continual Learning of Vision-and-Language Tasks Using Knowledge Distillation (ACL, 2023)
    
### Long-tail Learning
